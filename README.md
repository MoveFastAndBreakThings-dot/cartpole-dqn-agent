# ğŸ§  CartPole DQN Agent

![CartPole Screenshot](https://github.com/user-attachments/assets/f369e5ed-29c8-4892-b92b-53f84badbe7a)

A Deep Q-Network (DQN) implementation using **PyTorch** to solve the classic **CartPole-v1** balancing problem from OpenAI Gym. The agent learns to balance a pole on a cart by taking smart left/right actions based on observed environment states.

---

## ğŸš€ What This Project Does

This agent learns to balance a pole by:

- Observing the environment (cart position, velocity, pole angle, angular velocity)
- Predicting the best action using a neural network
- Receiving rewards and learning from them
- Updating its strategy through Deep Q-Learning

Eventually, the agent balances the pole **smoothly and efficiently**, even across the full width of the environment.

---

## âœ¨ Features

- âœ… Deep Q-Network (DQN) built with PyTorch  
- âœ… Experience Replay Buffer  
- âœ… Epsilon-Greedy Exploration Strategy  
- âœ… Target Network Updates  
- âœ… Live Agent Visualization using Pygame  
- âœ… Automatic Checkpointing & Resume Support  
- âœ… Trained on OpenAI Gymâ€™s CartPole-v1 environment

---

## ğŸ“š What I Learned

This project helped me understand and apply key Reinforcement Learning concepts:

- How agents interact with environments via actions and rewards  
- The mechanics of Deep Q-Learning  
- Exploration vs Exploitation (Epsilon-Greedy Policy)  
- Building a memory buffer and training loop  
- Optimizing models using PyTorch  
- Visualizing real-time agent behavior

---

## âš™ï¸ How It Works

### ğŸ§  Environment: CartPole-v1
The environment provides a 4D state:
- Cart Position  
- Cart Velocity  
- Pole Angle  
- Pole Angular Velocity

### ğŸ® Actions
- `0` â†’ Move Cart Left  
- `1` â†’ Move Cart Right

### ğŸ† Reward
- +1 reward per timestep the pole remains upright  
- Episode ends if the pole falls or the cart goes off-screen

---

## ğŸ§ª Training Summary

The agent is trained over 400 episodes. Initially, it explores randomly. Around episode 150, it begins balancing the pole intelligently. It eventually achieves a **maximum reward of 315**.

- Model checkpoints are saved after every episode in the `checkpoints/` directory.
- If training is interrupted, it **automatically resumes from the last saved episode**.

---

## ğŸ“ Project Structure

- `cartpole.py` â€“ Core DQN agent, environment handling, training logic  
- `GUI.py` â€“ Pygame-based visual interface to view agent behavior live  
- `run_trained_model.py` â€“ Runs the agent using saved model weights  
- `requirements.txt` â€“ All project dependencies

---

## ğŸ› ï¸ Setup & Installation

Make sure Python 3.x is installed. Then install all required libraries using:

```bash
pip install -r requirements.txt
```

This will automatically install all necessary packages including:
- `torch`
- `gym`
- `numpy`
- `pygame`

---

## â–¶ï¸ Usage

To **train the agent**, run:

```bash
python cartpole.py
```

To **run the trained agent**, use:

```bash
python run_trained_model.py
```

---

## ğŸ’¬ Feedback

If you have suggestions or run into issues, feel free to open an issue or reach out. I'm always looking to learn and improve this project!

---
